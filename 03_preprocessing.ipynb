{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac51423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy.sparse import hstack, save_npz, load_npz\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd90c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 11314\n",
      "Test samples: 7532\n",
      "Categories: 20\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, \n",
    "                                       remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, \n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X_train_full = newsgroups_train.data\n",
    "y_train_full = newsgroups_train.target\n",
    "X_test = newsgroups_test.data\n",
    "y_test = newsgroups_test.target\n",
    "category_names = newsgroups_train.target_names\n",
    "\n",
    "print(f\"Train samples: {len(X_train_full)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Categories: {len(category_names)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cabd001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split summary:\n",
      "Training set: 9616 samples (85.0%)\n",
      "Validation set: 1698 samples (15.0%)\n",
      "Test set: 7532 samples (holdout)\n",
      "\n",
      "         Split  Samples  Percentage\n",
      "0       Train     9616       51.02\n",
      "1  Validation     1698        9.01\n",
      "2        Test     7532       39.97\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full,\n",
    "    test_size=0.15,\n",
    "    stratify=y_train_full,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit summary:\")\n",
    "print(f\"Training set: {len(X_train)} samples ({len(X_train)/len(X_train_full)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(X_val)} samples ({len(X_val)/len(X_train_full)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} samples (holdout)\")\n",
    "\n",
    "split_df = pd.DataFrame({\n",
    "    'Split': ['Train', 'Validation', 'Test'],\n",
    "    'Samples': [len(X_train), len(X_val), len(X_test)],\n",
    "    'Percentage': [\n",
    "        len(X_train)/(len(X_train)+len(X_val)+len(X_test))*100,\n",
    "        len(X_val)/(len(X_train)+len(X_val)+len(X_test))*100,\n",
    "        len(X_test)/(len(X_train)+len(X_val)+len(X_test))*100\n",
    "    ]\n",
    "})\n",
    "print(\"\\n\", split_df.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d6c36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample BEFORE cleaning:\n",
      "Y'all lighten up on Harry, Skip'll be like that in a couple of years!!>\n",
      "\n",
      "Harry's a great personality.  He's the reason I like Cubs broadcasts.\n",
      "(It's certainly not the quality of the team).\n",
      "\n",
      "Chop Chop\n",
      "\n",
      "Michael Mule'\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sample AFTER cleaning:\n",
      "y'all lighten up on harry, skip'll be like that in a couple of years!!> harry's a great personality. he's the reason i like cubs broadcasts. (it's certainly not the quality of the team). chop chop michael mule'\n"
     ]
    }
   ],
   "source": [
    "def clean_text_basic(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '<URL>', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '<EMAIL>', text)\n",
    "    text = re.sub(r'\\d+', '<NUM>', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "sample_before = X_train[0][:300]\n",
    "sample_after = clean_text_basic(X_train[0])[:300]\n",
    "\n",
    "print(\"Sample BEFORE cleaning:\")\n",
    "print(sample_before)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nSample AFTER cleaning:\")\n",
    "print(sample_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea53b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning complete:\n",
      "Train: 9616 documents\n",
      "Val: 1698 documents\n",
      "Test: 7532 documents\n"
     ]
    }
   ],
   "source": [
    "X_train_cleaned = [clean_text_basic(doc) for doc in X_train]\n",
    "X_val_cleaned = [clean_text_basic(doc) for doc in X_val]\n",
    "X_test_cleaned = [clean_text_basic(doc) for doc in X_test]\n",
    "\n",
    "print(f\"\\nCleaning complete:\")\n",
    "print(f\"Train: {len(X_train_cleaned)} documents\")\n",
    "print(f\"Val: {len(X_val_cleaned)} documents\")\n",
    "print(f\"Test: {len(X_test_cleaned)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d164624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word TF-IDF Features:\n",
      "Vocabulary size: 50,000\n",
      "Train shape: (9616, 50000)\n",
      "Val shape: (1698, 50000)\n",
      "Test shape: (7532, 50000)\n",
      "Sparsity: 99.67%\n"
     ]
    }
   ],
   "source": [
    "tfidf_word = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=50000,\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words=None\n",
    ")\n",
    "\n",
    "X_train_tfidf_word = tfidf_word.fit_transform(X_train_cleaned)\n",
    "X_val_tfidf_word = tfidf_word.transform(X_val_cleaned)\n",
    "X_test_tfidf_word = tfidf_word.transform(X_test_cleaned)\n",
    "\n",
    "print(f\"Word TF-IDF Features:\")\n",
    "print(f\"Vocabulary size: {len(tfidf_word.vocabulary_):,}\")\n",
    "print(f\"Train shape: {X_train_tfidf_word.shape}\")\n",
    "print(f\"Val shape: {X_val_tfidf_word.shape}\")\n",
    "print(f\"Test shape: {X_test_tfidf_word.shape}\")\n",
    "print(f\"Sparsity: {(1.0 - X_train_tfidf_word.nnz / (X_train_tfidf_word.shape[0] * X_train_tfidf_word.shape[1]))*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca83dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Character TF-IDF Features:\n",
      "Vocabulary size: 50,000\n",
      "Train shape: (9616, 50000)\n",
      "Val shape: (1698, 50000)\n",
      "Test shape: (7532, 50000)\n",
      "Sparsity: 97.06%\n"
     ]
    }
   ],
   "source": [
    "tfidf_char = TfidfVectorizer(\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=50000,\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char'\n",
    ")\n",
    "\n",
    "X_train_tfidf_char = tfidf_char.fit_transform(X_train_cleaned)\n",
    "X_val_tfidf_char = tfidf_char.transform(X_val_cleaned)\n",
    "X_test_tfidf_char = tfidf_char.transform(X_test_cleaned)\n",
    "\n",
    "print(f\"\\nCharacter TF-IDF Features:\")\n",
    "print(f\"Vocabulary size: {len(tfidf_char.vocabulary_):,}\")\n",
    "print(f\"Train shape: {X_train_tfidf_char.shape}\")\n",
    "print(f\"Val shape: {X_val_tfidf_char.shape}\")\n",
    "print(f\"Test shape: {X_test_tfidf_char.shape}\")\n",
    "print(f\"Sparsity: {(1.0 - X_train_tfidf_char.nnz / (X_train_tfidf_char.shape[0] * X_train_tfidf_char.shape[1]))*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e164fa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hybrid Features (Word + Char):\n",
      "Train shape: (9616, 100000)\n",
      "Val shape: (1698, 100000)\n",
      "Test shape: (7532, 100000)\n",
      "Total features: 100,000\n"
     ]
    }
   ],
   "source": [
    "X_train_hybrid = hstack([X_train_tfidf_word, X_train_tfidf_char])\n",
    "X_val_hybrid = hstack([X_val_tfidf_word, X_val_tfidf_char])\n",
    "X_test_hybrid = hstack([X_test_tfidf_word, X_test_tfidf_char])\n",
    "\n",
    "print(f\"\\nHybrid Features (Word + Char):\")\n",
    "print(f\"Train shape: {X_train_hybrid.shape}\")\n",
    "print(f\"Val shape: {X_val_hybrid.shape}\")\n",
    "print(f\"Test shape: {X_test_hybrid.shape}\")\n",
    "print(f\"Total features: {X_train_hybrid.shape[1]:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0596693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Selection (Chi2, k=20000):\n",
      "Train shape: (9616, 20000)\n",
      "Val shape: (1698, 20000)\n",
      "Test shape: (7532, 20000)\n"
     ]
    }
   ],
   "source": [
    "chi2_selector = SelectKBest(chi2, k=20000)\n",
    "X_train_selected = chi2_selector.fit_transform(X_train_hybrid, y_train)\n",
    "X_val_selected = chi2_selector.transform(X_val_hybrid)\n",
    "X_test_selected = chi2_selector.transform(X_test_hybrid)\n",
    "\n",
    "print(f\"\\nFeature Selection (Chi2, k=20000):\")\n",
    "print(f\"Train shape: {X_train_selected.shape}\")\n",
    "print(f\"Val shape: {X_val_selected.shape}\")\n",
    "print(f\"Test shape: {X_test_selected.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff52874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature_Type  Dimensions  Train_NNZ  Sparsity_%\n",
      "0       Word TF-IDF (1,2)       50000    1603207       99.67\n",
      "1       Char TF-IDF (3,5)       50000   14142733       97.06\n",
      "2      Hybrid (Word+Char)      100000   15745940       98.36\n",
      "3  Selected (Chi2, k=20k)       20000    2750088       98.57\n"
     ]
    }
   ],
   "source": [
    "feature_summary = pd.DataFrame({\n",
    "    'Feature_Type': [\n",
    "        'Word TF-IDF (1,2)',\n",
    "        'Char TF-IDF (3,5)',\n",
    "        'Hybrid (Word+Char)',\n",
    "        'Selected (Chi2, k=20k)'\n",
    "    ],\n",
    "    'Dimensions': [\n",
    "        X_train_tfidf_word.shape[1],\n",
    "        X_train_tfidf_char.shape[1],\n",
    "        X_train_hybrid.shape[1],\n",
    "        X_train_selected.shape[1]\n",
    "    ],\n",
    "    'Train_NNZ': [\n",
    "        X_train_tfidf_word.nnz,\n",
    "        X_train_tfidf_char.nnz,\n",
    "        X_train_hybrid.nnz,\n",
    "        X_train_selected.nnz\n",
    "    ],\n",
    "    'Sparsity_%': [\n",
    "        (1.0 - X_train_tfidf_word.nnz / (X_train_tfidf_word.shape[0] * X_train_tfidf_word.shape[1])) * 100,\n",
    "        (1.0 - X_train_tfidf_char.nnz / (X_train_tfidf_char.shape[0] * X_train_tfidf_char.shape[1])) * 100,\n",
    "        (1.0 - X_train_hybrid.nnz / (X_train_hybrid.shape[0] * X_train_hybrid.shape[1])) * 100,\n",
    "        (1.0 - X_train_selected.nnz / (X_train_selected.shape[0] * X_train_selected.shape[1])) * 100\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(feature_summary.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96ee9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz('results/X_train_hybrid.npz', X_train_hybrid)\n",
    "save_npz('results/X_val_hybrid.npz', X_val_hybrid)\n",
    "save_npz('results/X_test_hybrid.npz', X_test_hybrid)\n",
    "\n",
    "np.save('results/y_train.npy', y_train)\n",
    "np.save('results/y_val.npy', y_val)\n",
    "np.save('results/y_test.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9815688b",
   "metadata": {},
   "source": [
    "## **Step 3 Analysis**\n",
    "\n",
    "### **Key Findings:**\n",
    "\n",
    "#### **1. Data Splits - Perfect Distribution**\n",
    "- **Train**: 9,616 samples (51.02%) \n",
    "- **Validation**: 1,698 samples (9.01%) \n",
    "- **Test**: 7,532 samples (39.97%) - **untouched holdout** \n",
    "- Stratified split preserves class distribution\n",
    "\n",
    "***\n",
    "\n",
    "#### **2. Text Cleaning - Effective Normalization**\n",
    "**Before:**\n",
    "- Mixed case: \"Y'all\", \"Harry\"\n",
    "- Natural language preserved\n",
    "\n",
    "**After:**\n",
    "- Lowercase: \"y'all\", \"harry\"\n",
    "- URLs → `<URL>`, Emails → `<EMAIL>`, Numbers → `<NUM>`\n",
    "- Whitespace normalized\n",
    "\n",
    "**Assessment**: Minimal but effective cleaning preserves semantic information while normalizing noise.\n",
    "\n",
    "***\n",
    "\n",
    "#### **3. Feature Extraction - High Quality**\n",
    "\n",
    "| Feature Type | Dimensions | Non-Zero | Sparsity |\n",
    "|-------------|-----------|----------|----------|\n",
    "| **Word TF-IDF (1,2)** | 50,000 | 1.6M | **99.67%** |\n",
    "| **Char TF-IDF (3,5)** | 50,000 | 14.1M | **97.06%** |\n",
    "| **Hybrid** | 100,000 | 15.7M | **98.36%** |\n",
    "| **Selected (Chi2)** | 20,000 | 2.7M | **98.57%** |\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **Word Features (99.67% sparse)**:\n",
    "   - Unigrams + bigrams capture semantic meaning\n",
    "   - Hit max_features=50k limit (good vocabulary coverage)\n",
    "   - Very sparse → efficient storage\n",
    "\n",
    "2. **Character Features (97.06% sparse)**:\n",
    "   - 3-5 char n-grams capture morphology, misspellings\n",
    "   - **9x more non-zero entries** than word features\n",
    "   - Denser but still manageable\n",
    "\n",
    "3. **Hybrid (100k dims)**:\n",
    "   - Best of both worlds\n",
    "   - Word semantics + character robustness\n",
    "   - 15.7M non-zero entries across 9,616 docs = **~1,634 features per doc avg**\n",
    "\n",
    "4. **Feature Selection (20k dims)**:\n",
    "   - **80% reduction** in dimensions (100k → 20k)\n",
    "   - Chi2 selects most discriminative features\n",
    "   - Still maintains **98.57% sparsity**\n",
    "   - 2.7M non-zero = **~286 features per doc avg**\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Files Successfully Saved**\n",
    "Vectorizers saved (`tfidf_word_vectorizer.pkl`, `tfidf_char_vectorizer.pkl`, `chi2_selector.pkl`)  \n",
    "Feature matrices saved (`.npz` format - sparse efficient)  \n",
    "Labels saved (`.npy` format)\n",
    "\n",
    "***\n",
    "\n",
    "## Preprocessing went **well** because:\n",
    "\n",
    "1. **Optimal Feature Space**: 100k hybrid → 20k selected is the sweet spot\n",
    "2. **High Sparsity**: >98% means efficient computation\n",
    "3. **Balanced Splits**: 85/15 train/val + holdout test is standard\n",
    "4. **Reproducibility**: All artifacts saved properly\n",
    "\n",
    "***\n",
    "\n",
    "## **Next Step: Step 4 - Baseline Modeling**\n",
    "\n",
    "**We will:**\n",
    "\n",
    "1. **Train 4 baseline models** on different feature sets:\n",
    "   - Logistic Regression\n",
    "   - Naive Bayes\n",
    "   - Linear SVM\n",
    "   - SGD Classifier\n",
    "\n",
    "2. **Compare feature representations**:\n",
    "   - Word-only TF-IDF\n",
    "   - Char-only TF-IDF\n",
    "   - Hybrid (Word+Char)\n",
    "   - Selected (20k best features)\n",
    "\n",
    "3. **10-Fold Stratified CV** for robust evaluation\n",
    "\n",
    "4. **Metrics to track**:\n",
    "   - Macro-F1 (primary)\n",
    "   - Weighted-F1\n",
    "   - Accuracy\n",
    "   - Per-class F1 scores\n",
    "   - Training time\n",
    "\n",
    "5. **Generate outputs**:\n",
    "   - CV score comparison table\n",
    "   - Confusion matrix for best model\n",
    "   - Classification report\n",
    "   - Feature importance analysis\n",
    "\n",
    "***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
